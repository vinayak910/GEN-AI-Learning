Architecture of transformer
---------------------------


contains : Encoder and decoder

Encoder
--------

The input texual data will be converted into vector and then make it a context vector by collecting info and relation to that input text.

Steps in encoder
1. input embedding : convert input text data into numbers
2. positional encoding : define position of text
3. Multi-head attension: What if the same word is used in 2 sentences with different context. So for this self attention mechanism is used . It finds relation of word with each word , and then generate a value 
4. Add and norm : we will add something to output generated by multi head then normalize it.
5. Feed Forward : understands complexity and patterns.(ANN , Multi layer percepton and other things in deep learning)
6. add and norm .

The Nx defines the number of layer of the encoder from step 1 to step 6. In chatgpt 3 , there are 96 layers.
Same goes for decoder

Now after all these steps , the encoder gives the resultant context vector. Now that context vector will be feed to decoder. which will predict the next word


refer to definition of encoder only and decoder only and other definitions.


Decoder architecture
---------------------

refer to process in jupyter notebook for better understanding of decoders , some of the steps in process are explained below


1. Input

2. Token embedding : word will be represented as vectors
3. Postitional encoding

4.Multi head attention : 

- the decoder uses a mechanism called "multi-head attention" to generate the output sequence. 
- it contains multipe self attension . where each self attension will focus on different words of input text such as verbs , object etc.parallely . 

Decoder Self Attension:

The self attention will not only input sequence but also will be applied on already generated output.

5.Masked Multi Head Attention 
- is used to allow the decoder to attend to the previous output tokens, while preventing it from seeing future tokens that have not yet been generated. 
- This is important because the decoder should only use information from previous tokens to generate the next token in the sequence.

6. Neural network : 

- after multi head attension , the data will be feeded to neural network. which will understand the complexity and patterns.

7. Output layer : 
- contains the probabilty of different prediction words generated by decoder. THe one with the highest probabilty will be chosen for next word.

- for selecting a next word there ar two methods	
	a) Greedy decoding : THe one with the highest probabilty will be chosen for next word
	b) beam search: refer to jupyter notebook


===========================================================================================================================================================

How input is preprocessed?

1. Tokenization 
------------

- Tokenization is the process of breaking down text into smaller units called tokens, which can be individual words, or even individual characters based on some rules.
- these rules can be defined by you or even can use predifned rules

different tokenization techinques
----------------------------------

- Word Based Tokenization
- character based tokenization

- Open AI faced problem while tokenizing using the above techniques ie large vocabulary size and less meaningful individual tokens. 
- So instead of this , open AI used different technique ie subword-based-tokenization.


Open Ai uses Byte Pair encoding and Google uses SentencePiece encoding. They are both types of subword tokenization.


===========================================================================================================================================================

2. Embeddeding : 
------------------
- making words a vector

-Token Embedding:
-----------------
 Once the text is tokenized, each token is mapped to a unique numerical ID in the vocabulary. This allows the model to represent each token as a fixed-length vector, which can be processed by the model.

- After self attention , there is one more type of embedding ie contexual embedding


- contexual embedding:
-----------------------
- vector with the same context will be same ,vector with different context will be different

Ex : bank vector dimension value will be different when using with context of money. and bank vector dimension will be different when using with context of river.




===========================================================================================================================================================

refer to 2. steps of encoder and decoder for better understanding of flow of data in encoder and decoder


