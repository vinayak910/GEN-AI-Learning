
- large language models have very vast knowledge as it is train on vast amount of data.
- but this knowledge is od no use unless and until they are able to answer or trained on our data.
- we cant use directly in our model . that's why we need to fine tune them (writing code or using frameworks)
- so that we can use for our usecases
- langchain , llma are few frameworks.


Some tools
------------

- you.com - Provide gpt 4 for free
- pi.ai - if you want to know the current scenario
- abs.perplexility.ai : provide the videos , links for a question you asked
- groq.com : when run llma2 on cpu ,it will be very slow. But this site uses then  uses lpu which makes the answer faster. It doesnt matter even if the model has 70 billion parameters
- ollama.com : can download this locally , and can pull different llm into the system and chat with it.
You can run ollama , by using this cmd in cmd prompt : ollama pull model_name.

refer to jupyter notebook collection to see sir study


Agents
-------

you specify a particular work to person with certain tools. That person will use the tools to perform a task is called agent.

the big Gen AI companies , they ar creating an agent , a person just like an agent. They are giving some tasks and tool to that person , and telling them to perform it.

Example of agent : 
1. Crew Ai :  here you can create different agents which are assigned different task . such as making am agent - write , and assigning a work to that agent

2. devika : indian product , used to build whole software using prompts. describe the use case 

3. MetaGPT
4. suno.ai : used for creating song



Generative AI
--------------

IN the traditional mls , we feed the data , and then prediction was made from that model

But in Gen AI we feed huge data, but it isnt used for prediction . instead it is used to generate more data/images.

GEN Ai , doesnt only mean llms , chatgpt . It includes any mdoel that generate a new content , new data. 

different GEN AI models : 

* Generative Adversarial Networks (GANs):
	+ Consist of a generator and a discriminator.
	+ Generator creates new data, while discriminator distinguishes between real and generated data.
	+ Trained together in an adversarial process.
	+ Generator gets better at creating realistic data, and discriminator gets better at distinguishing between real and generated data.

* Autoregressive Models:
	+ Neural network that can generate new data by predicting the next data point based on previous data points.
	+ Break down a sequence of data into individual data points.
	+ Predict each data point one at a time.
	+ Often used for tasks such as language modeling.

* Variational Autoencoders (VAEs):
	+ Neural network that can learn to generate new data by encoding and decoding data in a probabilistic manner.
	+ Consist of an encoder and a decoder.
	+ Encoder maps the input data to a probabilistic latent space.
	+ Decoder maps the latent space back to the input data.
	+ Trained to minimize the difference between the input data and the reconstructed data.
	+ Regularizes the latent space to follow a Gaussian distribution.
	+ Encourages the VAE to learn a smooth and continuous latent space.
	+ Used for tasks such as data generation and data interpolation.
        + both autoencoders and VAEs are neural networks that can learn to encode and decode data. Autoencoders learn a compact representation of the input        data, while VAEs learn a distribution over the latent space. This allows VAEs to generate new data by sampling from the latent space distribution, while autoencoders can only generate data that is similar to the input data.




GEN AI is not new
------------------

example 1 : google translation came way before LLMs and is the example of generative ai .

back then instead of LLM , the thing called Statistical Machine Translation(SMT) is used. Refer to jupyter notebook

example 2:
- Hey siri , hey cortona are also example of GEN AI.In these HHM models used for tasks like speech recognition.
- Supervised learning , nlp is also used in siri 
refer jupyter notebook . 

Story
------

* GPT (Generative Pretrained Transformer) is a powerful language model developed by OpenAI.
* GPT is trained on a vast corpus of text data, allowing it to generate human-like text based on a given prompt.
* GPT is a versatile model, capable of performing a wide range of natural language processing tasks.
* A key component of GPT is the decoder, a type of neural network component used in natural language processing tasks.
* Decoders take in a sequence of tokens (such as words or characters) and generate a new sequence of tokens, one token at a time.
* OpenAI developed ChatGPT, a model specifically designed for conversational tasks.
* ChatGPT is trained on conversational data, allowing it to generate more natural and coherent responses in a conversational setting.
* Google developed Bard, a conversational AI designed to help users complete tasks by providing personalized and contextually relevant information.
* OpenAI released GPT 3.0, which boasted even more impressive language generation capabilities than its predecessor.
* OpenAI released GPT-4, the most advanced version of GPT yet, with improved capabilities in areas such as reasoning, common sense, and creativity.
* The potential applications of language models are vast, from improving customer service to enhancing the creative process.
* The future of language models is bright, full of promise and possibility.



The concept of neural networks, a type of machine learning model inspired by the structure and function of the human brain, has been around for several decades. However, it wasn't until the 1980s that they began to be used for natural language processing (NLP) tasks. Early neural network models, such as recurrent neural networks (RNNs), were limited in their ability to process long sequences of text, as they processed tokens sequentially. This meant that they were not very efficient at handling tasks that required understanding the context of a sentence or a paragraph.

In 2017, a significant breakthrough was made in the field of NLP with the introduction of the transformer architecture. A paper titled "Attention is All You Need" was published, which introduced the transformer architecture. Transformers are particularly well-suited for NLP tasks, as they are able to process sequences of tokens in parallel, rather than sequentially. This allows transformers to process long sequences of text more efficiently than other neural network architectures.

Following the introduction of transformers, large language models began to emerge. Large language models are neural network models that have been trained on a vast amount of text data. These models are capable of generating human-like text based on a given prompt, and can be used for a wide range of NLP tasks, such as text classification, summarization, and translation.

One of the first large language models to gain widespread attention was BERT (Bidirectional Encoder Representations from Transformers), which was released in 2018. BERT is a transformer-based model that was trained on a large corpus of text data, allowing it to generate human-like text based on a given prompt. BERT was a significant improvement over previous models as it was able to understand the context of a sentence by looking at the words that come before and after it, a technique called bidirectional encoding.

In 2019, OpenAI released GPT-2 (Generative Pretrained Transformer 2), a large language model that was trained on an even larger corpus of text data than BERT. GPT-2 was capable of generating highly realistic and coherent text based on a given prompt. GPT-2 was able to generate text that was so realistic that OpenAI decided not to release the training data and the full model to the public due to concerns about misuse.

Later in 2019, Google released BERT's successor, BERT-2, which was an even more advanced version of the model. BERT-2 was capable of performing a wider range of NLP tasks than BERT, and was also faster and more efficient.

In 2020, OpenAI released GPT-3 (Generative Pretrained Transformer 3), a large language model that was even more advanced than GPT-2. GPT-3 was trained on an even larger corpus of text data than its predecessor, and was capable of generating highly realistic and coherent text based on a given prompt. GPT-3 was able to generate text that was so realistic that it was able to pass the Turing test, a test to determine whether a machine can exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.

Also in 2020, OpenAI released a variant of the GPT model called ChatGPT, which was specifically designed for conversational tasks. ChatGPT was fine-tuned on conversational data, allowing it to generate more natural and coherent responses in a conversational setting.

In 2021, Google released Bard, a conversational AI that is designed to help users complete tasks, such as planning a trip or finding a recipe, by providing personalized and contextually relevant information. Bard uses a combination of neural network models, including transformers, to understand the context of a conversation and provide relevant information.

In summary, the concept of neural networks has been around for several decades, but it wasn't until the introduction of transformers in 2017 that large language models began to emerge. Large language models are neural network models that have been trained on a vast amount of text data, and are capable of generating human-like text based on a given prompt. BERT, GPT-2, BERT-2, GPT-3, ChatGPT, and Bard are all examples of large language models that have been developed over time, with each iteration building on the capabilities of the previous one. These models have been instrumental in advancing the field of NLP and have opened up new possibilities for applications in various industries such as customer service, education, and entertainment.





Different Generative AI
-------------------------

refer to jupyter notebook

1. GAN

2. Variantional Autoencoders(VAE)

autoencoders :
- contains 2 neural networks ie encoders and decoders
- the data feed to encoder is bring down to latent space(dimensionality reduction) with keeping features importance , and then decoders decode the image from features

Variational :

- contains 2 neural networks encoders and decoders
- Encoder: Similar to traditional autoencoders, the encoder network in VAEs maps the input data to a latent space representation. However, instead of directly outputting a single point in the latent space, the encoder outputs parameters of a probability distribution (typically Gaussian) that describes the latent space.

- Decoder: The decoder network in VAEs takes samples from the latent space distribution (represented by the parameters outputted by the encoder) and generates output data. This sampling process introduces stochasticity, allowing VAEs to generate diverse variations of output data.
- useful in scenario like making a different drugs discovery

3. Autoregressive models :

- used in time series and sequence generation 
- generate from predicting on its previous outputs
- LLM models are based on these.



refer to tree diagram


Large language models : 
-------------------------

- large because trained on large data
- language : because it generates contextually relevant content

definition : 
- powerful AI model that uses deep learning techniques based on neural networks to understand and generate human like text
- they were trained to do only prediction of the next word. But how they are answering questions , machine translation and more 
- When llms understand how to predict next word , then they are supervised fine tuned ie trained on large corpus of question and answers and other , hence the ability of answering question , machine translation etc.
- Base model is fine tuned for different tasks and then you can also fine tune the already fined tuned model for your tasks. you can use platforms like hugging face etc.


Note : GPT 4 has 10^12 parameters and humans have 10^14 parameters. (year by year we are covering the bridge)


Some questions to answer
-------------------------

1. How did we get from single purpose system like google translate to Chatgpt?
2. Whats the core technology behind chatgpt ? Is it risky?
3. What's the future going to look like? should we be worried


- Main technology behind the llms are transformer.In todays world transformer is the base model behind any llm.



